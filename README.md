# ğŸš€ Waqar Chat AI â€“ RAG-Based Full Stack AI Application
A full-stack AI application built using Retrieval Augmented Generation (RAG).

This project allows users to upload documents (PDF / MP3), index them using embeddings, and interact with them through a modern ChatGPT-style interface.

## ğŸ“Œ Features
ğŸ“„ PDF document ingestion

ğŸ§ Audio (MP3) transcription

ğŸ§  Embedding-based similarity search

ğŸ¤– Gemini-powered response generation

ğŸ’¬ Multi-chat system

ğŸ—‚ Rename & delete chats

ğŸŒ™ Dark mode toggle

ğŸ’¾ Chat persistence (30 days)

ğŸ¨ Modern ChatGPT-style UI (React + Bootstrap)

## ğŸ— Project Architecture
Frontend (React)
        â¬‡
FastAPI Backend
        â¬‡
Embedding Model (Sentence Transformers)
        â¬‡
Vector Store (pickle)
        â¬‡
Similarity Search (Cosine Similarity)
        â¬‡
Gemini LLM API
        â¬‡
## Response Returned to Frontend
ğŸ§  Backend Setup (FastAPI + RAG)
ğŸ“ Step 1: Create Backend Folder

## cmd Command
```bash
mkdir gemini-chat-backend
cd gemini-chat-backend

## ğŸ Step 2: Create Virtual Environment
`python -m venv venv`
## Activate Environment
### Windows
(venv\Scripts\activate)

### Mac/Linux
(source venv/bin/activate)
ğŸ“¦ Step 3: Install Required Libraries
pip install fastapi uvicorn python-dotenv
pip install sentence-transformers
pip install scikit-learn
pip install PyPDF2
pip install openai-whisper
pip install google-generativeai
pip install python-multipart
Or Create requirements.txt
fastapi
uvicorn
python-dotenv
sentence-transformers
scikit-learn
PyPDF2
openai-whisper
google-generativeai
python-multipart
Then run:

pip install -r requirements.txt
ğŸ” Step 4: Create .env File
Create a file named:

.env
Add your Gemini API key:

GEMINI_API_KEY=your_actual_api_key_here
MODEL_NAME=gemini-1.5-flash
ğŸ§© Step 5: Load Environment Variables (config.py)
import os
from dotenv import load_dotenv
import google.generativeai as genai

load_dotenv()

API_KEY = os.getenv("GEMINI_API_KEY")
MODEL_NAME = os.getenv("MODEL_NAME")

genai.configure(api_key=API_KEY)
client = genai.GenerativeModel(MODEL_NAME)
ğŸ“‚ Step 6: Add Documents
Create a folder:

documents/
Add your:

PDF files

MP3 files

â–¶ï¸ Step 7: Run Backend
uvicorn main:app --reload
Backend runs at:

http://127.0.0.1:8000
ğŸ” How Backend Works
Reads documents

Extracts text

Converts text â†’ embeddings

Stores embeddings in vectorstore.pkl

When a question is asked:

Convert question â†’ embedding

Perform cosine similarity

Retrieve best matching chunk

Send context + question to Gemini

Return generated answer

ğŸ’» Frontend Setup (React + Bootstrap)
ğŸ“ Step 1: Create React App
npx create-react-app waqar-chat-frontend
cd waqar-chat-frontend
ğŸ“¦ Step 2: Install Required Libraries
npm install bootstrap
npm install bootstrap-icons
npm install react-markdown
npm install uuid
ğŸ§© Step 3: Import Bootstrap
In src/index.js:

import 'bootstrap/dist/css/bootstrap.min.css';
import 'bootstrap-icons/font/bootstrap-icons.css';
ğŸ”— Step 4: Connect to Backend
In App.js:

fetch("http://127.0.0.1:8000/ask", {
  method: "POST",
  headers: { "Content-Type": "application/json" },
  body: JSON.stringify({ question: input }),
});
â–¶ï¸ Step 5: Run Frontend
npm start
App runs at:

http://localhost:3000
ğŸ§  How Model â€œTrainingâ€ Works
This project does NOT fine-tune the LLM.

Instead, it uses Retrieval Augmented Generation (RAG):

Documents are embedded

Embeddings are stored

User question is embedded

Most similar chunk is retrieved

Retrieved chunk is sent to Gemini

Gemini generates contextual answer

No retraining required.

ğŸ“Š Why RAG Instead of Fine-Tuning?
Fine-Tuning	RAG
Expensive	Cost-effective
Static knowledge	Uses fresh documents
Needs retraining	Just re-index documents
Hard to update	Easy to update
ğŸ›  Commands Summary
Backend
python -m venv venv
venv\Scripts\activate
pip install -r requirements.txt
uvicorn main:app --reload
Frontend
npx create-react-app waqar-chat-frontend
npm install bootstrap bootstrap-icons react-markdown uuid
npm start
ğŸ”® Future Improvements
MongoDB persistent storage

User authentication

Cloud deployment

Streaming responses

Hybrid search (Vector + BM25)

ğŸ‘¨â€ğŸ’» Author
Waqar Ahmad
AI Full Stack Developer

â­ If You Like This Project
Give it a â­ on GitHub!
